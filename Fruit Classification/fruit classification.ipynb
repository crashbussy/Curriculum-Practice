{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "421a270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##需要重新安装import torch_utils as tu\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "\n",
    "import albumentations\n",
    "from albumentations import pytorch as AT\n",
    "\n",
    "#below are all from https://github.com/seefun/TorchUtils, thanks seefun to provide such useful tools\n",
    "#import torch_utils as tu\n",
    "import torch.utils as tu\n",
    "FOLD = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8c664f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple_6', 'apple_braeburn_1', 'apple_crimson_snow_1', 'apple_golden_1', 'apple_golden_2', 'apple_golden_3', 'apple_granny_smith_1', 'apple_hit_1', 'apple_pink_lady_1', 'apple_red_1', 'apple_red_2', 'apple_red_3', 'apple_red_delicios_1', 'apple_red_yellow_1', 'apple_rotten_1', 'cabbage_white_1', 'carrot_1', 'cucumber_1', 'cucumber_3', 'eggplant_violet_1', 'pear_1', 'pear_3', 'zucchini_1', 'zucchini_dark_1']\n"
     ]
    }
   ],
   "source": [
    "#导入数据集\n",
    "#其中transform已删除\n",
    "\n",
    "train_data=torchvision.datasets .ImageFolder (root='./fruits-360-original-size/Training' )\n",
    "val_data=torchvision.datasets .ImageFolder (root='./fruits-360-original-size/Validation' )\n",
    "test_ds=torchvision.datasets .ImageFolder (root='./fruits-360-original-size/Test' )\n",
    "print(train_data.classes)  #输出train_data里面的文件夹名称\n",
    "\n",
    "#制作dataloader、即可迭代的数据装载器\n",
    "batchsize=500   #每次抓取的数据数量\n",
    "train_loader=DataLoader(train_data,batch_size=batchsize,shuffle=True,num_workers=1)\n",
    "val_loader=DataLoader(val_data,batch_size=batchsize,shuffle=True,num_workers=1)\n",
    "test_loader = DataLoader(test_ds, batch_size=batchsize, shuffle=False, num_workers=2)\n",
    "#如何获得读入的图片总数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f25bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50d', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10c07e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3071d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(lr, nb, epochs, model_name='resnet50d', MIXUP=0.1):\n",
    "    #？输入项nb\n",
    "  #mixup_fn = tu.Mixup(prob=MIXUP, switch_prob=0.0, onehot=True, label_smoothing=0.05, num_classes=len(train_loader))\n",
    "  model = timm.create_model(model_name, pretrained=True)\n",
    "    #这一步设置了什么？具体内容  model.cuda  model.train()等用法\n",
    "  model.fc = nn.Linear(model.fc.in_features, len(train_loader))\n",
    "  nn.init.xavier_uniform_(model.fc.weight)\n",
    "  model.cuda()\n",
    "    #？\n",
    "\n",
    "  params_1x = [param for name, param in model.named_parameters()\n",
    "              if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "\n",
    "  optimizer = torch.optim.AdamW([{'params': params_1x},\n",
    "                                    {'params': model.fc.parameters(),\n",
    "                                      'lr': lr*10}],\n",
    "                                  lr=lr, weight_decay=2e-4)\n",
    "#loss_fn  损失函数 (Loss Function)\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  loss_fn_test = F.cross_entropy\n",
    "#用交叉熵区别两个概率的区别\n",
    "\n",
    "  lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*nb, eta_min=lr/20)\n",
    "    #？\n",
    "  return model, optimizer, loss_fn, loss_fn_test, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e9e876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fold0...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-048203457bb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mlogit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1201\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;31m# instantiate since we don't know how to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"D:\\Ananconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 86, in default_collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "import ttach as tta\n",
    "device = 'cuda'\n",
    "save_dir = './gdrive/MyDrive/kaggle_leave_classification/'\n",
    "#map from idx to string？？\n",
    "####去除了labelmap部分\n",
    "EPOCHS = 10\n",
    "#mixup是什么？用在哪里？\n",
    "MIXUP = 0.1\n",
    "\n",
    " \n",
    "scaler = torch.cuda.amp.GradScaler() # for AMP training Automatic Mixed Precision,\n",
    "#AMP可以在神经网络推理过程中，针对不同的层，采用不同的数据精度进行计算，从而实现节省显存和加快速度的目的\n",
    "#有了epochs为什么还要fold循环5次？？？因为做5则交叉验证，省数据\n",
    "for fold in range(FOLD):\n",
    "  print(f'Start Fold{fold}...')\n",
    "\n",
    "  train_dl=train_loader\n",
    "  val_dl=val_loader\n",
    "  model, optimizer, loss_fn, loss_fn_test, lr_scheduler = get_learner(3e-4, len(train_dl), EPOCHS, model_name='resnet50d', MIXUP=MIXUP)\n",
    "  model_name = f'5fold_test_fold{fold}'\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  train_accus = []\n",
    "  val_accus = []\n",
    "  best_accu = 0\n",
    "  best_loss = float('inf')\n",
    "  lrs = []\n",
    "  for epoch in range(EPOCHS):\n",
    "          val_accu = 0\n",
    "          train_accu = 0\n",
    "          train_losses_tmp = []\n",
    "          #Train\n",
    "          model.train()\n",
    "          t_inf = 0\n",
    "          #for x, y in train_dl:\n",
    "              #if MIXUP:\n",
    "               # x, y = mixup_fn(x, y)\n",
    "             # x, y = x.to(device), y.to(device)\n",
    "              #Forward\n",
    "              #torch.cuda.amp.autocast短训练时间，降低存储需求，作用在哪一类代码上？\n",
    "             # with torch.cuda.amp.autocast():\n",
    "                #pred = model(x)\n",
    "               # loss = loss_fn(pred, y)\n",
    "                #mixup？\n",
    "                \n",
    "              #Backward\n",
    "              #loss.backward()\n",
    "              #optimizer.step()\n",
    "              #scaler.scale(loss).backward()\n",
    "              #scaler.step(optimizer)\n",
    "             # scaler.update()\n",
    "             # lr_scheduler.step()\n",
    "              #optimizer.zero_grad()\n",
    "              #Statistics\n",
    "              #lrs.append(optimizer.param_groups[0]['lr']) #group 0,1,2 share the learning rate\n",
    "             # train_losses_tmp.append(loss.data.item())\n",
    "              #pred_labels = torch.argmax(pred.data, dim=1)\n",
    "              #y_labels = torch.argmax(y.data, dim=1) if MIXUP else y.data\n",
    "              #train_accu += (pred_labels==y_labels).float().sum()\n",
    "          #t_inf /= len(train_dl)\n",
    "          #train_losses.append(np.mean(np.array(train_losses_tmp)))\n",
    "         ###### train_accu /= n_train\n",
    "          #train_accus.append(train_accu.data.item())\n",
    "\n",
    "            \n",
    "            #loss是训练集的损失值，val_loss是测试集的损失值\n",
    "          #Validation\n",
    "          val_losses_tmp = []\n",
    "          model.eval()\n",
    "          with torch.no_grad():\n",
    "            for x, y in val_dl:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logit = model(x)\n",
    "                val_loss = loss_fn_test(logit, y) \n",
    "                val_losses_tmp.append(val_loss.data.item())\n",
    "                pred = torch.argmax(logit.data, dim=1)\n",
    "                val_accu += (pred==y.data).float().sum()\n",
    "          val_loss = np.mean(np.array(val_losses_tmp))\n",
    "          val_losses.append(val_loss)\n",
    "          #######val_accu /= n_val\n",
    "          val_accus.append(val_accu.data.item())\n",
    "          print('fold', fold, 'epoch', epoch, 'train_loss', train_losses[epoch], 'val_loss', val_losses[epoch], 'val_accu', val_accu, 'train_accu', train_accu, 'lr[0]', lrs[-1])\n",
    "          if save_dir is not None:\n",
    "              if val_accu == best_accu:\n",
    "                  if val_loss < best_loss: #never satisfied\n",
    "                      checkpoint = {\"model\": model.state_dict()}\n",
    "                      torch.save(checkpoint, os.path.join(save_dir,f'{model_name}_best.pth'))\n",
    "                      print(f'Stored a new best model in {save_dir}')\n",
    "                      best_loss = val_loss\n",
    "              elif val_accu > best_accu:\n",
    "                  checkpoint = {\"model\": model.state_dict()}\n",
    "                  torch.save(checkpoint, os.path.join(save_dir,f'{model_name}_best.pth'))\n",
    "                  print(f'Stored a new best model in {save_dir}')\n",
    "                  best_accu = val_accu\n",
    "              '''\n",
    "              if epoch == EPOCHS - 1:\n",
    "                  checkpoint = {\"model\": model.state_dict()}\n",
    "                  torch.save(checkpoint, os.path.join(save_dir,f'{model_name}_last.pth'))\n",
    "                  print(f'Stored the last model in {save_dir}')\n",
    "              '''\n",
    "  #test time\n",
    "  model.eval()\n",
    "  tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.flip_transform(),  merge_mode='mean')\n",
    "  tta_model.eval()\n",
    "  res = []\n",
    "  for x in test_dl:\n",
    "      x = x.to(device)\n",
    "      logit = tta_model(x)\n",
    "      pred = torch.argmax(logit.data, dim=1).cpu().numpy()\n",
    "      for i in range(len(pred)):\n",
    "        res.append(labelmap_inverse[pred[i]])\n",
    "        #append在列表末尾添加新的对象\n",
    "\n",
    "  #test_csv.insert(1, 'label', res)\n",
    "  #test_csv.to_csv(f'submission_e50{model_name}_fold{fold}.csv'), index=False)\n",
    "    #如何保存，以csv的形式保存？？？\n",
    "  #print('test cvs is saved')\n",
    "#输出分类结果？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(train_accus)\n",
    "plt.plot(val_accus)\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.ylim(0, 1.3)\n",
    "plt.legend(['train_accus', 'val_accus', 'train_losses', 'val_losses'])\n",
    "#plt.show()\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.savefig(f'{model_name}_acc98d34.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
